{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df6e4bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polars.config.Config"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute duckdb query and transform the result into a parquet file\n",
    "import duckdb\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "from snapshot import snapshot_query\n",
    "from typing import List\n",
    "\n",
    "pl.Config.set_tbl_rows(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49634337",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_from = \"2013-01-01\"\n",
    "train_upto = \"2016-08-15\"\n",
    "today = \"2016-08-15\"\n",
    "\n",
    "snapshot_query = snapshot_query.format(date_from=train_from, date_upto=train_upto)\n",
    "\n",
    "with duckdb.connect(database=\"../dbcore/data/core.db\", read_only=True) as con:\n",
    "    train_snapshot_df = con.execute(snapshot_query).pl()\n",
    "train_snapshot_df.write_parquet(\"../../data/favorita_dataset/snapshots/train_snapshot.parquet\", compression=\"zstd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19d511af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1_316, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>c_date</th><th>product_id</th><th>store_id</th><th>log_units_sold</th><th>h1_log_units_sold</th></tr><tr><td>date</td><td>i32</td><td>i32</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>2013-01-02</td><td>1</td><td>1</td><td>3.555348</td><td>3.912023</td></tr><tr><td>2013-01-03</td><td>1</td><td>1</td><td>3.912023</td><td>3.135494</td></tr><tr><td>2013-01-04</td><td>1</td><td>1</td><td>3.135494</td><td>3.401197</td></tr><tr><td>2013-01-05</td><td>1</td><td>1</td><td>3.401197</td><td>3.332205</td></tr><tr><td>2013-01-06</td><td>1</td><td>1</td><td>3.332205</td><td>3.258097</td></tr><tr><td>2013-01-07</td><td>1</td><td>1</td><td>3.258097</td><td>2.944439</td></tr><tr><td>2013-01-08</td><td>1</td><td>1</td><td>2.944439</td><td>3.526361</td></tr><tr><td>2013-01-09</td><td>1</td><td>1</td><td>3.526361</td><td>3.433987</td></tr><tr><td>2013-01-10</td><td>1</td><td>1</td><td>3.433987</td><td>2.639057</td></tr><tr><td>2013-01-11</td><td>1</td><td>1</td><td>2.639057</td><td>2.944439</td></tr><tr><td>2013-01-12</td><td>1</td><td>1</td><td>2.944439</td><td>2.833213</td></tr><tr><td>2013-01-13</td><td>1</td><td>1</td><td>2.833213</td><td>3.258097</td></tr><tr><td>2013-01-14</td><td>1</td><td>1</td><td>3.258097</td><td>3.218876</td></tr><tr><td>2013-01-15</td><td>1</td><td>1</td><td>3.218876</td><td>3.135494</td></tr><tr><td>2013-01-16</td><td>1</td><td>1</td><td>3.135494</td><td>3.367296</td></tr><tr><td>2013-01-17</td><td>1</td><td>1</td><td>3.367296</td><td>2.944439</td></tr><tr><td>2013-01-18</td><td>1</td><td>1</td><td>2.944439</td><td>3.367296</td></tr><tr><td>2013-01-19</td><td>1</td><td>1</td><td>3.367296</td><td>3.367296</td></tr><tr><td>2013-01-20</td><td>1</td><td>1</td><td>3.367296</td><td>2.890372</td></tr><tr><td>2013-01-21</td><td>1</td><td>1</td><td>2.890372</td><td>2.484907</td></tr><tr><td>2013-01-22</td><td>1</td><td>1</td><td>2.484907</td><td>3.496508</td></tr><tr><td>2013-01-23</td><td>1</td><td>1</td><td>3.496508</td><td>3.713572</td></tr><tr><td>2013-01-24</td><td>1</td><td>1</td><td>3.713572</td><td>2.995732</td></tr><tr><td>2013-01-25</td><td>1</td><td>1</td><td>2.995732</td><td>3.367296</td></tr><tr><td>2013-01-26</td><td>1</td><td>1</td><td>3.367296</td><td>2.302585</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>2016-07-22</td><td>1</td><td>1</td><td>3.496508</td><td>2.833213</td></tr><tr><td>2016-07-23</td><td>1</td><td>1</td><td>2.833213</td><td>3.178054</td></tr><tr><td>2016-07-24</td><td>1</td><td>1</td><td>3.178054</td><td>3.135494</td></tr><tr><td>2016-07-25</td><td>1</td><td>1</td><td>3.135494</td><td>3.091042</td></tr><tr><td>2016-07-26</td><td>1</td><td>1</td><td>3.091042</td><td>3.401197</td></tr><tr><td>2016-07-27</td><td>1</td><td>1</td><td>3.401197</td><td>2.995732</td></tr><tr><td>2016-07-28</td><td>1</td><td>1</td><td>2.995732</td><td>3.526361</td></tr><tr><td>2016-07-29</td><td>1</td><td>1</td><td>3.526361</td><td>3.178054</td></tr><tr><td>2016-07-30</td><td>1</td><td>1</td><td>3.178054</td><td>3.218876</td></tr><tr><td>2016-07-31</td><td>1</td><td>1</td><td>3.218876</td><td>3.555348</td></tr><tr><td>2016-08-01</td><td>1</td><td>1</td><td>3.555348</td><td>3.044522</td></tr><tr><td>2016-08-02</td><td>1</td><td>1</td><td>3.044522</td><td>3.526361</td></tr><tr><td>2016-08-03</td><td>1</td><td>1</td><td>3.526361</td><td>2.890372</td></tr><tr><td>2016-08-04</td><td>1</td><td>1</td><td>2.890372</td><td>3.044522</td></tr><tr><td>2016-08-05</td><td>1</td><td>1</td><td>3.044522</td><td>2.890372</td></tr><tr><td>2016-08-06</td><td>1</td><td>1</td><td>2.890372</td><td>2.995732</td></tr><tr><td>2016-08-07</td><td>1</td><td>1</td><td>2.995732</td><td>3.218876</td></tr><tr><td>2016-08-08</td><td>1</td><td>1</td><td>3.218876</td><td>3.178054</td></tr><tr><td>2016-08-09</td><td>1</td><td>1</td><td>3.178054</td><td>3.367296</td></tr><tr><td>2016-08-10</td><td>1</td><td>1</td><td>3.367296</td><td>3.135494</td></tr><tr><td>2016-08-11</td><td>1</td><td>1</td><td>3.135494</td><td>3.367296</td></tr><tr><td>2016-08-12</td><td>1</td><td>1</td><td>3.367296</td><td>2.70805</td></tr><tr><td>2016-08-13</td><td>1</td><td>1</td><td>2.70805</td><td>2.995732</td></tr><tr><td>2016-08-14</td><td>1</td><td>1</td><td>2.995732</td><td>2.995732</td></tr><tr><td>2016-08-15</td><td>1</td><td>1</td><td>2.995732</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1_316, 5)\n",
       "┌────────────┬────────────┬──────────┬────────────────┬───────────────────┐\n",
       "│ c_date     ┆ product_id ┆ store_id ┆ log_units_sold ┆ h1_log_units_sold │\n",
       "│ ---        ┆ ---        ┆ ---      ┆ ---            ┆ ---               │\n",
       "│ date       ┆ i32        ┆ i32      ┆ f64            ┆ f64               │\n",
       "╞════════════╪════════════╪══════════╪════════════════╪═══════════════════╡\n",
       "│ 2013-01-02 ┆ 1          ┆ 1        ┆ 3.555348       ┆ 3.912023          │\n",
       "│ 2013-01-03 ┆ 1          ┆ 1        ┆ 3.912023       ┆ 3.135494          │\n",
       "│ 2013-01-04 ┆ 1          ┆ 1        ┆ 3.135494       ┆ 3.401197          │\n",
       "│ 2013-01-05 ┆ 1          ┆ 1        ┆ 3.401197       ┆ 3.332205          │\n",
       "│ 2013-01-06 ┆ 1          ┆ 1        ┆ 3.332205       ┆ 3.258097          │\n",
       "│ 2013-01-07 ┆ 1          ┆ 1        ┆ 3.258097       ┆ 2.944439          │\n",
       "│ 2013-01-08 ┆ 1          ┆ 1        ┆ 2.944439       ┆ 3.526361          │\n",
       "│ 2013-01-09 ┆ 1          ┆ 1        ┆ 3.526361       ┆ 3.433987          │\n",
       "│ 2013-01-10 ┆ 1          ┆ 1        ┆ 3.433987       ┆ 2.639057          │\n",
       "│ 2013-01-11 ┆ 1          ┆ 1        ┆ 2.639057       ┆ 2.944439          │\n",
       "│ 2013-01-12 ┆ 1          ┆ 1        ┆ 2.944439       ┆ 2.833213          │\n",
       "│ 2013-01-13 ┆ 1          ┆ 1        ┆ 2.833213       ┆ 3.258097          │\n",
       "│ 2013-01-14 ┆ 1          ┆ 1        ┆ 3.258097       ┆ 3.218876          │\n",
       "│ 2013-01-15 ┆ 1          ┆ 1        ┆ 3.218876       ┆ 3.135494          │\n",
       "│ 2013-01-16 ┆ 1          ┆ 1        ┆ 3.135494       ┆ 3.367296          │\n",
       "│ 2013-01-17 ┆ 1          ┆ 1        ┆ 3.367296       ┆ 2.944439          │\n",
       "│ 2013-01-18 ┆ 1          ┆ 1        ┆ 2.944439       ┆ 3.367296          │\n",
       "│ 2013-01-19 ┆ 1          ┆ 1        ┆ 3.367296       ┆ 3.367296          │\n",
       "│ 2013-01-20 ┆ 1          ┆ 1        ┆ 3.367296       ┆ 2.890372          │\n",
       "│ 2013-01-21 ┆ 1          ┆ 1        ┆ 2.890372       ┆ 2.484907          │\n",
       "│ 2013-01-22 ┆ 1          ┆ 1        ┆ 2.484907       ┆ 3.496508          │\n",
       "│ 2013-01-23 ┆ 1          ┆ 1        ┆ 3.496508       ┆ 3.713572          │\n",
       "│ 2013-01-24 ┆ 1          ┆ 1        ┆ 3.713572       ┆ 2.995732          │\n",
       "│ 2013-01-25 ┆ 1          ┆ 1        ┆ 2.995732       ┆ 3.367296          │\n",
       "│ 2013-01-26 ┆ 1          ┆ 1        ┆ 3.367296       ┆ 2.302585          │\n",
       "│ …          ┆ …          ┆ …        ┆ …              ┆ …                 │\n",
       "│ 2016-07-22 ┆ 1          ┆ 1        ┆ 3.496508       ┆ 2.833213          │\n",
       "│ 2016-07-23 ┆ 1          ┆ 1        ┆ 2.833213       ┆ 3.178054          │\n",
       "│ 2016-07-24 ┆ 1          ┆ 1        ┆ 3.178054       ┆ 3.135494          │\n",
       "│ 2016-07-25 ┆ 1          ┆ 1        ┆ 3.135494       ┆ 3.091042          │\n",
       "│ 2016-07-26 ┆ 1          ┆ 1        ┆ 3.091042       ┆ 3.401197          │\n",
       "│ 2016-07-27 ┆ 1          ┆ 1        ┆ 3.401197       ┆ 2.995732          │\n",
       "│ 2016-07-28 ┆ 1          ┆ 1        ┆ 2.995732       ┆ 3.526361          │\n",
       "│ 2016-07-29 ┆ 1          ┆ 1        ┆ 3.526361       ┆ 3.178054          │\n",
       "│ 2016-07-30 ┆ 1          ┆ 1        ┆ 3.178054       ┆ 3.218876          │\n",
       "│ 2016-07-31 ┆ 1          ┆ 1        ┆ 3.218876       ┆ 3.555348          │\n",
       "│ 2016-08-01 ┆ 1          ┆ 1        ┆ 3.555348       ┆ 3.044522          │\n",
       "│ 2016-08-02 ┆ 1          ┆ 1        ┆ 3.044522       ┆ 3.526361          │\n",
       "│ 2016-08-03 ┆ 1          ┆ 1        ┆ 3.526361       ┆ 2.890372          │\n",
       "│ 2016-08-04 ┆ 1          ┆ 1        ┆ 2.890372       ┆ 3.044522          │\n",
       "│ 2016-08-05 ┆ 1          ┆ 1        ┆ 3.044522       ┆ 2.890372          │\n",
       "│ 2016-08-06 ┆ 1          ┆ 1        ┆ 2.890372       ┆ 2.995732          │\n",
       "│ 2016-08-07 ┆ 1          ┆ 1        ┆ 2.995732       ┆ 3.218876          │\n",
       "│ 2016-08-08 ┆ 1          ┆ 1        ┆ 3.218876       ┆ 3.178054          │\n",
       "│ 2016-08-09 ┆ 1          ┆ 1        ┆ 3.178054       ┆ 3.367296          │\n",
       "│ 2016-08-10 ┆ 1          ┆ 1        ┆ 3.367296       ┆ 3.135494          │\n",
       "│ 2016-08-11 ┆ 1          ┆ 1        ┆ 3.135494       ┆ 3.367296          │\n",
       "│ 2016-08-12 ┆ 1          ┆ 1        ┆ 3.367296       ┆ 2.70805           │\n",
       "│ 2016-08-13 ┆ 1          ┆ 1        ┆ 2.70805        ┆ 2.995732          │\n",
       "│ 2016-08-14 ┆ 1          ┆ 1        ┆ 2.995732       ┆ 2.995732          │\n",
       "│ 2016-08-15 ┆ 1          ┆ 1        ┆ 2.995732       ┆ null              │\n",
       "└────────────┴────────────┴──────────┴────────────────┴───────────────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_snapshot_df.filter(product_id=1, store_id=1).select(\n",
    "        pl.col(\"c_date\"),\n",
    "    pl.col(\"product_id\"),\n",
    "    pl.col(\"store_id\"),\n",
    "    pl.col(\"log_units_sold\"),\n",
    "    pl.col(\"h1_log_units_sold\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09e2210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_target_df(dataset: pl.DataFrame, target_cols: List[str], agg_level: List[str]) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Get target DataFrame for training.\n",
    "    \n",
    "    Args:\n",
    "        dataset (pl.LazyFrame): LazyFrame containing training data.\n",
    "        target_cols (str): Column name for the target variable.\n",
    "    Returns:\n",
    "        pl.LazyFrame: LazyFrame with target variable.\n",
    "    \"\"\"\n",
    "    # fill nulls in target columns using forward fill strategy\n",
    "\n",
    "    tmp = dataset.group_by(agg_level, maintain_order=True).agg(\n",
    "        pl.col(name).fill_null(strategy=\"forward\") for name in target_cols\n",
    "    ).explode(target_cols)\n",
    "\n",
    "    return tmp.select(target_cols)\n",
    "\n",
    "# get target set for train set and validation set\n",
    "def pop_columns(df: pl.DataFrame, col_names: List[str]) -> pl.DataFrame:\n",
    "    return pl.DataFrame(\n",
    "        [\n",
    "            df.drop_in_place(col_name)\n",
    "            for col_name in col_names\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def drop_columns(df: pl.DataFrame, col_names: List[str]) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Drop specified columns from the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pl.DataFrame): DataFrame to drop columns from.\n",
    "        col_names (List[str]): List of column names to drop.\n",
    "        \n",
    "    Returns:\n",
    "        pl.DataFrame: DataFrame with specified columns dropped.\n",
    "    \"\"\"\n",
    "    for col_name in col_names:\n",
    "        df.drop_in_place(col_name)\n",
    "\n",
    "def feature_engineering(train_lzdf: pl.LazyFrame, target: str, horizon:int, agg_level: List[str], fill_nulls: bool=False) -> pl.LazyFrame:\n",
    "    \"\"\"\n",
    "    Feature engineering for sales data.\n",
    "    \n",
    "    Args:\n",
    "        train_lzdf (pl.LazyFrame): LazyFrame containing sales data.\n",
    "        \n",
    "    Returns:\n",
    "        pl.LazyFrame: LazyFrame with engineered features.\n",
    "    \"\"\"\n",
    "\n",
    "    # 2. Add calendar features\n",
    "    train_lzdf = train_lzdf.with_columns([\n",
    "        pl.col(\"c_date\").dt.weekday().alias(\"dayofweek\"),\n",
    "        pl.col(\"c_date\").dt.day().alias(\"dayofmonth\"),\n",
    "        pl.col(\"c_date\").dt.ordinal_day().alias(\"dayofyear\"),\n",
    "        pl.col(\"c_date\").dt.week().alias(\"weekofyear\"),\n",
    "        pl.col(\"c_date\").dt.month().alias(\"month\"),\n",
    "        pl.col(\"c_date\").dt.year().alias(\"year\"),\n",
    "    ])\n",
    "\n",
    "    # 4. Add rolling features over the previous 3, 7, 14, and 28 days\n",
    "    for window in [3, 7, 14, 21, 28]:\n",
    "        # Only consider window before current c_date to avoid data leakage. This is done by using closed='left'\n",
    "        tmp = train_lzdf.rolling('c_date', period=f'{window}d', closed='right', group_by=agg_level).agg(\n",
    "            # 4.1 Calculate rolling mean, median, std, min, max, and ewm_mean to log_units_sold\n",
    "            pl.col(\"log_units_sold\").mean().alias(f\"mean_{window}d_log_units_sold\"),\n",
    "            pl.col(\"log_units_sold\").median().alias(f\"median_{window}d_log_units_sold\"),\n",
    "            pl.col(\"log_units_sold\").std().alias(f\"std_{window}d_log_units_sold\"),\n",
    "            pl.col(\"log_units_sold\").min().alias(f\"min_{window}d_log_units_sold\"),\n",
    "            pl.col(\"log_units_sold\").max().alias(f\"max_{window}d_log_units_sold\"),\n",
    "            pl.col(\"log_units_sold\").ewm_mean(alpha=0.9, adjust=True).last().alias(f\"ewm_{window}d_log_units_sold\"),\n",
    "            pl.col(\"log_units_sold\").diff().mean().alias(f\"diff_mean_{window}d_log_units_sold\"),\n",
    "        ).with_columns(\n",
    "            # 4.3 Calculate ratio of max to mean. Useful to identify outliers\n",
    "            (pl.col(f\"max_{window}d_log_units_sold\") / pl.col(f\"mean_{window}d_log_units_sold\"))\n",
    "                .alias(f\"max_mean_ratio_{window}d_log_units_sold\")\n",
    "        )\n",
    "        # train_lzdf = pl.concat([train_lzdf, tmp], how=\"horizontal\", parallel=True)\n",
    "        train_lzdf = train_lzdf.join(tmp, on=agg_level + [\"c_date\"], how=\"left\")\n",
    "\n",
    "    # 5. Add weekday rolling mean. e.i. mean of the same weekday in the past 4 weeks\n",
    "    # TODO: No funciona bien, el primer problema es que el rolling semana no es correcto, checa over()\n",
    "    # el segundo problema es que no se esta considerando el dia de la semana, se deben de crear columns para cada dia de la semana\n",
    "    # for weekday in range(1, 8):\n",
    "    #     train_lzdf = train_lzdf.with_columns(\n",
    "    #         pl.col(\"log_units_sold\")\n",
    "    #             .rolling_mean_by('date', window_size=f\"3w\", closed=\"right\")\n",
    "    #             .over(agg_level + [\"weekday\"])\n",
    "    #             .alias(f\"mean_3w_{weekday}wd_log_units_sold\")\n",
    "    #     )\n",
    "\n",
    "    # 5. Add yearly rolling ewm. e.i. ewm of the same day in the past 3 years\n",
    "    feature_name = \"ewm_3y_log_units_sold\"\n",
    "\n",
    "    tmp = train_lzdf.rolling(\"c_date\", period=\"3y\", closed=\"right\", group_by=agg_level+[\"dayofyear\"]).agg(\n",
    "        pl.col.log_units_sold.ewm_mean(alpha=0.9).last().alias(feature_name)\n",
    "    ).with_columns(\n",
    "        (pl.col(\"c_date\").dt.offset_by(\"1y\").dt.offset_by(f\"-{h}d\")).alias(f\"h{h}_c_date\") \\\n",
    "        for h in range(1, horizon+1)\n",
    "    )\n",
    "    \n",
    "    for h in range(1, horizon + 1):\n",
    "        train_lzdf = train_lzdf.join(\n",
    "            tmp.select(\n",
    "                *agg_level,\n",
    "                f\"h{h}_c_date\",\n",
    "                feature_name\n",
    "            ).rename({feature_name: f\"h{h}_{feature_name}\"}),\n",
    "            left_on=agg_level+[\"c_date\"],\n",
    "            right_on=agg_level+[f\"h{h}_c_date\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "\n",
    "    # # 8. Finally fills null values with 0\n",
    "    # if fill_nulls:\n",
    "    #     train_lzdf = train_lzdf.fill_null(0)\n",
    "    \n",
    "    # 9. Filter by date range\n",
    "    return train_lzdf.sort(by=[\"product_id\", \"store_id\", \"c_date\"])\n",
    "    \n",
    "\n",
    "# Get columns for horizons\n",
    "def apply_horizon_shifting(train_dataset: pl.DataFrame, horizons: int, agg_level: List[str]):\n",
    "    # Add predictions columns for horizons\n",
    "    for horizon in range(1, horizons + 1):\n",
    "        tmp = train_dataset.select(\n",
    "            *agg_level,\n",
    "            pl.col(\"date\") - pl.duration(days=horizon),\n",
    "            pl.col(\"log_units_sold\").alias(f\"h{horizon}_log_units_sold\"),\n",
    "        )\n",
    "\n",
    "        train_dataset = train_dataset.join(tmp, on=agg_level+[\"date\"], how=\"left\")\n",
    "    return train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac4056b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df(df: pl.DataFrame, path: str):\n",
    "    # choossing the best compression for a LightGBM model\n",
    "    df.write_parquet(\n",
    "        path,\n",
    "        compression=\"zstd\",\n",
    "        # row_group_size=1000000,  # Uncomment if you need to optimize for large datasets\n",
    "        # partition_by=[\"store_id\", \"product_id\"],  # Uncomment if you want to optimize\n",
    "    )\n",
    "\n",
    "def train_pipeline(\n",
    "    dataset: pl.DataFrame,\n",
    "    target: str,\n",
    "    horizon: int,\n",
    "    agg_level: List[str],\n",
    "):\n",
    "    target_cols = [f\"h{h}_{target}\" for h in range(1, horizon + 1)]\n",
    "    target_df = extract_target_df(dataset, target_cols, agg_level)\n",
    "    drop_columns(dataset, target_cols)\n",
    "    input_df = feature_engineering(dataset, target, horizon, agg_level)\n",
    "    dates_df = pop_columns(input_df, [\"c_date\"])\n",
    "\n",
    "    save_df(target_df, f\"../../data/favorita_dataset/output/train_target.parquet\")\n",
    "    save_df(input_df, f\"../../data/favorita_dataset/output/train_input.parquet\")\n",
    "    save_df(dates_df, f\"../../data/favorita_dataset/output/train_dates.parquet\")\n",
    "\n",
    "    return input_df, target_df, dates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c78107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pl.read_parquet(\"../../data/favorita_dataset/snapshots/train_snapshot.parquet\")\n",
    "x, y, c = train_pipeline(\n",
    "    dataset=dataset,\n",
    "    target=\"log_units_sold\",\n",
    "    horizon=7,\n",
    "    agg_level=[\"product_id\", \"store_id\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f5a9f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1_316, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>product_id</th><th>store_id</th><th>log_units_sold</th><th>h1_log_units_sold</th></tr><tr><td>i32</td><td>i32</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>7</td><td>5</td><td>3.295837</td><td>2.70805</td></tr><tr><td>7</td><td>5</td><td>2.70805</td><td>2.70805</td></tr><tr><td>7</td><td>5</td><td>2.70805</td><td>2.890372</td></tr><tr><td>7</td><td>5</td><td>2.890372</td><td>3.135494</td></tr><tr><td>7</td><td>5</td><td>3.135494</td><td>2.397895</td></tr><tr><td>7</td><td>5</td><td>2.397895</td><td>1.94591</td></tr><tr><td>7</td><td>5</td><td>1.94591</td><td>2.302585</td></tr><tr><td>7</td><td>5</td><td>2.302585</td><td>2.564949</td></tr><tr><td>7</td><td>5</td><td>2.564949</td><td>3.135494</td></tr><tr><td>7</td><td>5</td><td>3.135494</td><td>3.258097</td></tr><tr><td>7</td><td>5</td><td>3.258097</td><td>3.332205</td></tr><tr><td>7</td><td>5</td><td>3.332205</td><td>2.484907</td></tr><tr><td>7</td><td>5</td><td>2.484907</td><td>2.397895</td></tr><tr><td>7</td><td>5</td><td>2.397895</td><td>3.091042</td></tr><tr><td>7</td><td>5</td><td>3.091042</td><td>2.772589</td></tr><tr><td>7</td><td>5</td><td>2.772589</td><td>2.197225</td></tr><tr><td>7</td><td>5</td><td>2.197225</td><td>2.772589</td></tr><tr><td>7</td><td>5</td><td>2.772589</td><td>3.091042</td></tr><tr><td>7</td><td>5</td><td>3.091042</td><td>2.564949</td></tr><tr><td>7</td><td>5</td><td>2.564949</td><td>2.484907</td></tr><tr><td>7</td><td>5</td><td>2.484907</td><td>2.944439</td></tr><tr><td>7</td><td>5</td><td>2.944439</td><td>2.397895</td></tr><tr><td>7</td><td>5</td><td>2.397895</td><td>2.70805</td></tr><tr><td>7</td><td>5</td><td>2.70805</td><td>3.091042</td></tr><tr><td>7</td><td>5</td><td>3.091042</td><td>3.044522</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>7</td><td>5</td><td>3.496508</td><td>3.044522</td></tr><tr><td>7</td><td>5</td><td>3.044522</td><td>3.178054</td></tr><tr><td>7</td><td>5</td><td>3.178054</td><td>3.044522</td></tr><tr><td>7</td><td>5</td><td>3.044522</td><td>2.639057</td></tr><tr><td>7</td><td>5</td><td>2.639057</td><td>2.772589</td></tr><tr><td>7</td><td>5</td><td>2.772589</td><td>2.772589</td></tr><tr><td>7</td><td>5</td><td>2.772589</td><td>3.044522</td></tr><tr><td>7</td><td>5</td><td>3.044522</td><td>3.526361</td></tr><tr><td>7</td><td>5</td><td>3.526361</td><td>3.091042</td></tr><tr><td>7</td><td>5</td><td>3.091042</td><td>2.564949</td></tr><tr><td>7</td><td>5</td><td>2.564949</td><td>3.258097</td></tr><tr><td>7</td><td>5</td><td>3.258097</td><td>2.302585</td></tr><tr><td>7</td><td>5</td><td>2.302585</td><td>2.484907</td></tr><tr><td>7</td><td>5</td><td>2.484907</td><td>3.044522</td></tr><tr><td>7</td><td>5</td><td>3.044522</td><td>3.401197</td></tr><tr><td>7</td><td>5</td><td>3.401197</td><td>3.526361</td></tr><tr><td>7</td><td>5</td><td>3.526361</td><td>3.332205</td></tr><tr><td>7</td><td>5</td><td>3.332205</td><td>2.302585</td></tr><tr><td>7</td><td>5</td><td>2.302585</td><td>2.890372</td></tr><tr><td>7</td><td>5</td><td>2.890372</td><td>3.178054</td></tr><tr><td>7</td><td>5</td><td>3.178054</td><td>3.091042</td></tr><tr><td>7</td><td>5</td><td>3.091042</td><td>2.833213</td></tr><tr><td>7</td><td>5</td><td>2.833213</td><td>3.218876</td></tr><tr><td>7</td><td>5</td><td>3.218876</td><td>3.044522</td></tr><tr><td>7</td><td>5</td><td>3.044522</td><td>3.044522</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1_316, 4)\n",
       "┌────────────┬──────────┬────────────────┬───────────────────┐\n",
       "│ product_id ┆ store_id ┆ log_units_sold ┆ h1_log_units_sold │\n",
       "│ ---        ┆ ---      ┆ ---            ┆ ---               │\n",
       "│ i32        ┆ i32      ┆ f64            ┆ f64               │\n",
       "╞════════════╪══════════╪════════════════╪═══════════════════╡\n",
       "│ 7          ┆ 5        ┆ 3.295837       ┆ 2.70805           │\n",
       "│ 7          ┆ 5        ┆ 2.70805        ┆ 2.70805           │\n",
       "│ 7          ┆ 5        ┆ 2.70805        ┆ 2.890372          │\n",
       "│ 7          ┆ 5        ┆ 2.890372       ┆ 3.135494          │\n",
       "│ 7          ┆ 5        ┆ 3.135494       ┆ 2.397895          │\n",
       "│ 7          ┆ 5        ┆ 2.397895       ┆ 1.94591           │\n",
       "│ 7          ┆ 5        ┆ 1.94591        ┆ 2.302585          │\n",
       "│ 7          ┆ 5        ┆ 2.302585       ┆ 2.564949          │\n",
       "│ 7          ┆ 5        ┆ 2.564949       ┆ 3.135494          │\n",
       "│ 7          ┆ 5        ┆ 3.135494       ┆ 3.258097          │\n",
       "│ 7          ┆ 5        ┆ 3.258097       ┆ 3.332205          │\n",
       "│ 7          ┆ 5        ┆ 3.332205       ┆ 2.484907          │\n",
       "│ 7          ┆ 5        ┆ 2.484907       ┆ 2.397895          │\n",
       "│ 7          ┆ 5        ┆ 2.397895       ┆ 3.091042          │\n",
       "│ 7          ┆ 5        ┆ 3.091042       ┆ 2.772589          │\n",
       "│ 7          ┆ 5        ┆ 2.772589       ┆ 2.197225          │\n",
       "│ 7          ┆ 5        ┆ 2.197225       ┆ 2.772589          │\n",
       "│ 7          ┆ 5        ┆ 2.772589       ┆ 3.091042          │\n",
       "│ 7          ┆ 5        ┆ 3.091042       ┆ 2.564949          │\n",
       "│ 7          ┆ 5        ┆ 2.564949       ┆ 2.484907          │\n",
       "│ 7          ┆ 5        ┆ 2.484907       ┆ 2.944439          │\n",
       "│ 7          ┆ 5        ┆ 2.944439       ┆ 2.397895          │\n",
       "│ 7          ┆ 5        ┆ 2.397895       ┆ 2.70805           │\n",
       "│ 7          ┆ 5        ┆ 2.70805        ┆ 3.091042          │\n",
       "│ 7          ┆ 5        ┆ 3.091042       ┆ 3.044522          │\n",
       "│ …          ┆ …        ┆ …              ┆ …                 │\n",
       "│ 7          ┆ 5        ┆ 3.496508       ┆ 3.044522          │\n",
       "│ 7          ┆ 5        ┆ 3.044522       ┆ 3.178054          │\n",
       "│ 7          ┆ 5        ┆ 3.178054       ┆ 3.044522          │\n",
       "│ 7          ┆ 5        ┆ 3.044522       ┆ 2.639057          │\n",
       "│ 7          ┆ 5        ┆ 2.639057       ┆ 2.772589          │\n",
       "│ 7          ┆ 5        ┆ 2.772589       ┆ 2.772589          │\n",
       "│ 7          ┆ 5        ┆ 2.772589       ┆ 3.044522          │\n",
       "│ 7          ┆ 5        ┆ 3.044522       ┆ 3.526361          │\n",
       "│ 7          ┆ 5        ┆ 3.526361       ┆ 3.091042          │\n",
       "│ 7          ┆ 5        ┆ 3.091042       ┆ 2.564949          │\n",
       "│ 7          ┆ 5        ┆ 2.564949       ┆ 3.258097          │\n",
       "│ 7          ┆ 5        ┆ 3.258097       ┆ 2.302585          │\n",
       "│ 7          ┆ 5        ┆ 2.302585       ┆ 2.484907          │\n",
       "│ 7          ┆ 5        ┆ 2.484907       ┆ 3.044522          │\n",
       "│ 7          ┆ 5        ┆ 3.044522       ┆ 3.401197          │\n",
       "│ 7          ┆ 5        ┆ 3.401197       ┆ 3.526361          │\n",
       "│ 7          ┆ 5        ┆ 3.526361       ┆ 3.332205          │\n",
       "│ 7          ┆ 5        ┆ 3.332205       ┆ 2.302585          │\n",
       "│ 7          ┆ 5        ┆ 2.302585       ┆ 2.890372          │\n",
       "│ 7          ┆ 5        ┆ 2.890372       ┆ 3.178054          │\n",
       "│ 7          ┆ 5        ┆ 3.178054       ┆ 3.091042          │\n",
       "│ 7          ┆ 5        ┆ 3.091042       ┆ 2.833213          │\n",
       "│ 7          ┆ 5        ┆ 2.833213       ┆ 3.218876          │\n",
       "│ 7          ┆ 5        ┆ 3.218876       ┆ 3.044522          │\n",
       "│ 7          ┆ 5        ┆ 3.044522       ┆ 3.044522          │\n",
       "└────────────┴──────────┴────────────────┴───────────────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.select(\n",
    "    # pl.col(\"c_date\"),\n",
    "    pl.col(\"product_id\"),\n",
    "    pl.col(\"store_id\"),\n",
    "    pl.col(\"log_units_sold\"),\n",
    "    y.get_column(\"h1_log_units_sold\"),\n",
    "    # pl.col(\"h1_log_units_sold\"),\n",
    ").filter(product_id=7, store_id=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "74810cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1316\n"
     ]
    }
   ],
   "source": [
    "sample = train_df.filter(pl.col.product_id == 1, pl.col.store_id == 1)\n",
    "\n",
    "print(sample.height)\n",
    "\n",
    "sample = sample.with_columns(\n",
    "    pl.col.c_date.dt.weekday().alias(\"dayofweek\"),\n",
    "    pl.col.c_date.dt.year().alias(\"year\"),\n",
    "    pl.col.c_date.dt.month().alias(\"month\"),\n",
    "    pl.col.c_date.dt.ordinal_day().alias(\"dayofyear\"),\n",
    "    pl.col.c_date.dt.day().alias(\"dayofmonth\"),\n",
    ").sort([\"product_id\", \"store_id\", \"c_date\"])\n",
    "\n",
    "tmp = sample.rolling(\"c_date\", period=\"3y\", closed=\"right\", group_by=[\"product_id\", \"store_id\", \"dayofyear\"]).agg(\n",
    "    pl.col.log_units_sold.ewm_mean(alpha=0.9).last().alias(\"ewm_3y_log_units_sold\")\n",
    ").with_columns(\n",
    "    (pl.col(\"c_date\").dt.offset_by(\"1y\").dt.offset_by(f\"-{h}d\")).alias(f\"h{h}_c_date\") for h in range(1,8)\n",
    ")#.drop(\"c_date\", \"dayofyear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff4abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "\n",
    "from typing import Tuple, List\n",
    "from datetime import date\n",
    "\n",
    "# def ewm(arr: pl.Series, alpha=0.9):\n",
    "#     if arr.len() == 0:\n",
    "#         return None\n",
    "#     weights = (1 - alpha) ** np.arange(arr.len()-1, -1, -1)\n",
    "#     return (arr * weights).sum() / weights.sum()\n",
    "\n",
    "def feature_engineering(snapshot_path: str, between: Tuple[date, date], target:str, agg_level: List[str], fill_nulls: bool=False) -> pl.LazyFrame:\n",
    "    \"\"\"\n",
    "    Feature engineering for sales data.\n",
    "    \n",
    "    Args:\n",
    "        sales_lzdf (pl.LazyFrame): LazyFrame containing sales data.\n",
    "        \n",
    "    Returns:\n",
    "        pl.LazyFrame: LazyFrame with engineered features.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Load\n",
    "    sales_lzdf = pl.scan_parquet(\n",
    "        snapshot_path,\n",
    "    ).sort(agg_level + [\"date\"]).with_columns([\n",
    "        pl.col(\"units_sold\").log1p(),\n",
    "        cs.boolean().cast(pl.Int8),  # Convert boolean to int for compatibility with LightGBM\n",
    "        cs.string().cast(pl.Categorical), # Convert string categories to categorical type\n",
    "    ]).rename({\"units_sold\": \"log_units_sold\"})  # Drop original units_sold column to avoid confusion\n",
    "\n",
    "    # 2. Add calendar features\n",
    "    sales_lzdf = sales_lzdf.with_columns([\n",
    "        pl.col(\"date\").dt.weekday().alias(\"weekday\"),\n",
    "        pl.col(\"date\").dt.month().alias(\"month\"),\n",
    "        pl.col(\"date\").dt.week().alias(\"weekofyear\"),\n",
    "        pl.col(\"date\").dt.ordinal_day().alias(\"dayofyear\"),\n",
    "    ]).with_columns(\n",
    "        cs.boolean().cast(pl.Int8)  # Convert boolean to int for compatibility with LightGBM\n",
    "    )\n",
    "\n",
    "    # # 3. Add lagged features for the previous 1, 3, 7, and 14 days\n",
    "    # for lag in [1, 3, 7, 14]:\n",
    "    #     # 3.1 This is a workaround for considering no registered sales on the lag day\n",
    "    #     # by shifting the date by the lag and joining on the shifted date\n",
    "    #     tmp = sales_lzdf.select(\n",
    "    #         *agg_level,\n",
    "    #         pl.col(\"date\") + pl.duration(days=lag),\n",
    "    #         pl.col(\"log_units_sold\").alias(f\"lag_{lag}d_log_units_sold\"),\n",
    "    #         # pl.col(\"is_on_promotion\").alias(f\"lag_{lag}d_is_on_promotion\"),\n",
    "    #     )\n",
    "        \n",
    "    #     sales_lzdf = sales_lzdf.join(tmp, on=agg_level+[\"date\"], how=\"left\")\n",
    "\n",
    "        # 3.2 This is the original way to add lags, but it will not consider no registered sales on the lag day\n",
    "        # sales_lzdf = sales_lzdf.with_columns(\n",
    "        #     pl.col(\"log_units_sold\")\n",
    "        #         .shift_by(lag, fill_value=0)  # Fill with 0 to avoid NaN\n",
    "        #         .over([\"product_id\", \"store_id\"], order_by=\"date\")\n",
    "        #         .alias(f\"lag_{lag}d_log_units_sold\")\n",
    "        # )\n",
    "\n",
    "    # 4. Add rolling features over the previous 3, 7, 14, and 28 days\n",
    "    for window in [3, 7, 14, 28]:\n",
    "        # Only consider window before current date to avoid data leakage. This is done by using closed='left'\n",
    "        tmp = sales_lzdf.rolling('date', period=f'{window}d', closed='right', group_by=agg_level).agg(\n",
    "            # 4.1 Calculate rolling mean, median, std, min, max, and ewm_mean to log_units_sold\n",
    "            pl.col(\"log_units_sold\").mean().alias(f\"mean_{window}d_log_units_sold\"),\n",
    "            pl.col(\"log_units_sold\").median().alias(f\"median_{window}d_log_units_sold\"),\n",
    "            pl.col(\"log_units_sold\").std().alias(f\"std_{window}d_log_units_sold\"),\n",
    "            pl.col(\"log_units_sold\").min().alias(f\"min_{window}d_log_units_sold\"),\n",
    "            pl.col(\"log_units_sold\").max().alias(f\"max_{window}d_log_units_sold\"),\n",
    "            pl.col(\"log_units_sold\").ewm_mean(alpha=0.9, adjust=True).last().alias(f\"ewm_{window}d_log_units_sold\"),\n",
    "            pl.col(\"log_units_sold\").diff().mean().alias(f\"diff_mean_{window}d_log_units_sold\"),\n",
    "        ).with_columns(\n",
    "            # 4.3 Calculate ratio of max to mean. Useful to identify outliers\n",
    "            (pl.col(f\"max_{window}d_log_units_sold\") / pl.col(f\"mean_{window}d_log_units_sold\"))\n",
    "                .alias(f\"max_mean_ratio_{window}d_log_units_sold\")\n",
    "        )\n",
    "        # sales_lzdf = pl.concat([sales_lzdf, tmp], how=\"horizontal\", parallel=True)\n",
    "        sales_lzdf = sales_lzdf.join(tmp, on=agg_level + [\"date\"], how=\"left\")\n",
    "\n",
    "        # sales_lzdf = sales_lzdf.with_columns(\n",
    "        #     # Calculate rolling mean\n",
    "        #     pl.col('log_units_sold') # Only consider window before current date to avoid data leakage\n",
    "        #         .rolling_mean_by('date', window_size=f\"{window}d\", closed=\"left\")\n",
    "        #         .over([\"product_id\", \"store_id\"])\n",
    "        #         .alias(f\"mean_{window}d_log_units_sold\"),\n",
    "        #     # Calculate rolling median\n",
    "        #     pl.col('log_units_sold') # Only consider window before current date to avoid data leakage\n",
    "        #         .rolling_median_by('date', window_size=f\"{window}d\", closed=\"left\")\n",
    "        #         .over([\"product_id\", \"store_id\"])\n",
    "        #         .alias(f\"median_{window}d_log_units_sold\"),\n",
    "        #     # Calculate rolling standard deviation\n",
    "        #     pl.col('log_units_sold') # Only consider window before current date to avoid data leakage\n",
    "        #         .rolling_std_by('date', window_size=f\"{window}d\", closed=\"left\")\n",
    "        #         .over([\"product_id\", \"store_id\"])\n",
    "        #         .alias(f\"std_{window}d_log_units_sold\"),\n",
    "        #     # Calculate rolling min\n",
    "        #     pl.col('log_units_sold') # Only consider window before current date to avoid data leakage\n",
    "        #         .rolling_min_by('date', window_size=f\"{window}d\", closed=\"left\")\n",
    "        #         .over([\"product_id\", \"store_id\"])\n",
    "        #         .alias(f\"min_{window}d_log_units_sold\"),\n",
    "        #     # Calculate rolling max\n",
    "        #     pl.col('log_units_sold') # Only consider window before current date to avoid data leakage\n",
    "        #         .rolling_max_by('date', window_size=f\"{window}d\", closed=\"left\")\n",
    "        #         .over([\"product_id\", \"store_id\"])\n",
    "        #         .alias(f\"max_{window}d_log_units_sold\"),\n",
    "        #     # # calculate rolling exponential weighted mean\n",
    "        #     # pl.col('log_units_sold') # Only consider window before current date to avoid data leakage\n",
    "        #     # .rolling_map(ewm, window_size=f\"{window}d\", closed=\"left\"),\n",
    "                # pl.col(\"is_on_promotion\")\n",
    "                #         .rolling_sum_by('date', window_size=f\"{window}d\", closed=\"left\")\n",
    "                #         .over(agg_level)\n",
    "                #         .alias(f\"sum_{window}d_is_on_promotion\")\n",
    "        # )\n",
    "\n",
    "    # 5. Add weekday rolling mean. e.i. mean of the same weekday in the past 4 weeks\n",
    "    for weeks in [1, 2, 3, 4]:\n",
    "        sales_lzdf = sales_lzdf.with_columns(\n",
    "            pl.col(\"log_units_sold\")\n",
    "                .rolling_mean_by('date', window_size=f\"{weeks}w\", closed=\"left\")\n",
    "                .over(agg_level + [\"weekday\"])\n",
    "                .alias(f\"mean_{weeks}w_log_units_sold\")\n",
    "        )\n",
    "\n",
    "    # 5. Add yearly rolling mean. e.i. mean of the same day in the past 4 years\n",
    "    for years in [1, 2, 3, 4]:\n",
    "        sales_lzdf = sales_lzdf.with_columns(\n",
    "            pl.col(\"log_units_sold\")\n",
    "                .rolling_mean_by('date', window_size=f\"{years}w\", closed=\"left\")\n",
    "                .over(agg_level + [\"dayofyear\"])\n",
    "                .alias(f\"mean_{years}y_log_units_sold\")\n",
    "        )\n",
    "\n",
    "    # 6. Add is_on_promotion column rolling sum after 16 days\n",
    "    for window in [3, 7, 14]:\n",
    "        tmp = sales_lzdf.rolling('date', period=f'{window}d', offset=\"0d\", closed='right', group_by=agg_level).agg(\n",
    "            pl.col(\"is_on_promotion\").sum().alias(f\"sum_next_{window}d_is_on_promotion\")\n",
    "        )\n",
    "        sales_lzdf = sales_lzdf.join(tmp, on=agg_level + [\"date\"], how=\"left\")\n",
    "\n",
    "    # 7. Join item features\n",
    "    # TODO: Esta concatenacion no deberia de estar aqui, deberia de ser otro paso del pipeline\n",
    "    products_lzdf = pl.scan_parquet(\n",
    "        \"../../data/favorita_dataset/subset/products.parquet\"\n",
    "    ).with_columns(\n",
    "        cs.boolean().cast(pl.Int8),  # Convert boolean to int for compatibility with LightGBM\n",
    "        cs.string().cast(pl.Categorical), # Convert string categories to categorical type\n",
    "    )\n",
    "\n",
    "    sales_lzdf = sales_lzdf.join(\n",
    "        products_lzdf,\n",
    "        on=\"product_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # 8. Finally fills null values with 0\n",
    "    if fill_nulls:\n",
    "        sales_lzdf = sales_lzdf.fill_null(0)\n",
    "    \n",
    "    # 9. Filter by date range\n",
    "    return sales_lzdf.filter(pl.col(\"date\").is_between(*between))\n",
    "    \n",
    "\n",
    "# Get columns for horizons\n",
    "def apply_horizon_shifting(train_dataset: pl.DataFrame, horizons: int, agg_level: List[str]):\n",
    "    # Add predictions columns for horizons\n",
    "    for horizon in range(1, horizons + 1):\n",
    "        tmp = train_dataset.select(\n",
    "            *agg_level,\n",
    "            pl.col(\"date\") - pl.duration(days=horizon),\n",
    "            pl.col(\"log_units_sold\").alias(f\"h{horizon}_log_units_sold\"),\n",
    "        )\n",
    "\n",
    "        train_dataset = train_dataset.join(tmp, on=agg_level+[\"date\"], how=\"left\")\n",
    "    return train_dataset\n",
    "\n",
    "    # # 7. Flatten for single model\n",
    "    # # let's expand for horizon 1–7\n",
    "    # horizons = []\n",
    "    # for h in range(1,2):\n",
    "    #     tmp = sales_lzdf.filter(\n",
    "    #         pl.col(\"date\") <= pl.date(\"2017-08-15\")\n",
    "    #     ).with_columns([\n",
    "    #         pl.lit(h).alias(\"horizon\")\n",
    "    #     ])\n",
    "    #     # align y label\n",
    "    #     tmp = tmp.with_columns(\n",
    "    #         pl.col(\"units_sold\")\n",
    "    #         .shift(-h, by=[\"store_nbr\", \"item_nbr\"])\n",
    "    #         .alias(\"target\")\n",
    "    #     )\n",
    "    #     horizons.append(tmp)\n",
    "\n",
    "    # train = pl.concat(horizons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a110507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 71)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>product_id</th><th>store_id</th><th>log_units_sold</th><th>product_group</th><th>next_1d_event_id</th><th>next_2d_event_id</th><th>next_3d_event_id</th><th>next_4d_event_id</th><th>next_5d_event_id</th><th>next_6d_event_id</th><th>next_7d_event_id</th><th>next_1d_is_on_promo</th><th>next_2d_is_on_promo</th><th>next_3d_is_on_promo</th><th>next_4d_is_on_promo</th><th>next_5d_is_on_promo</th><th>next_6d_is_on_promo</th><th>next_7d_is_on_promo</th><th>dayofweek</th><th>dayofmonth</th><th>dayofyear</th><th>weekofyear</th><th>month</th><th>year</th><th>mean_3d_log_units_sold</th><th>median_3d_log_units_sold</th><th>std_3d_log_units_sold</th><th>min_3d_log_units_sold</th><th>max_3d_log_units_sold</th><th>ewm_3d_log_units_sold</th><th>diff_mean_3d_log_units_sold</th><th>max_mean_ratio_3d_log_units_sold</th><th>mean_7d_log_units_sold</th><th>median_7d_log_units_sold</th><th>std_7d_log_units_sold</th><th>min_7d_log_units_sold</th><th>max_7d_log_units_sold</th><th>ewm_7d_log_units_sold</th><th>diff_mean_7d_log_units_sold</th><th>max_mean_ratio_7d_log_units_sold</th><th>mean_14d_log_units_sold</th><th>median_14d_log_units_sold</th><th>std_14d_log_units_sold</th><th>min_14d_log_units_sold</th><th>max_14d_log_units_sold</th><th>ewm_14d_log_units_sold</th><th>diff_mean_14d_log_units_sold</th><th>max_mean_ratio_14d_log_units_sold</th><th>mean_21d_log_units_sold</th><th>median_21d_log_units_sold</th><th>std_21d_log_units_sold</th><th>min_21d_log_units_sold</th><th>max_21d_log_units_sold</th><th>ewm_21d_log_units_sold</th><th>diff_mean_21d_log_units_sold</th><th>max_mean_ratio_21d_log_units_sold</th><th>mean_28d_log_units_sold</th><th>median_28d_log_units_sold</th><th>std_28d_log_units_sold</th><th>min_28d_log_units_sold</th><th>max_28d_log_units_sold</th><th>ewm_28d_log_units_sold</th><th>diff_mean_28d_log_units_sold</th><th>max_mean_ratio_28d_log_units_sold</th><th>h1_ewm_3y_log_units_sold</th><th>h2_ewm_3y_log_units_sold</th><th>h3_ewm_3y_log_units_sold</th><th>h4_ewm_3y_log_units_sold</th><th>h5_ewm_3y_log_units_sold</th><th>h6_ewm_3y_log_units_sold</th><th>h7_ewm_3y_log_units_sold</th></tr><tr><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>42130</td><td>42026</td><td>42027</td><td>42063</td><td>42064</td><td>42170</td><td>42066</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>37</td><td>0</td><td>0</td><td>0</td><td>37</td><td>0</td><td>0</td><td>0</td><td>37</td><td>0</td><td>0</td><td>0</td><td>37</td><td>0</td><td>0</td><td>0</td><td>35</td><td>0</td><td>0</td><td>0</td><td>35</td><td>0</td><td>0</td><td>0</td><td>35</td><td>0</td><td>0</td><td>0</td><td>35</td><td>0</td><td>0</td><td>0</td><td>35</td><td>0</td><td>0</td><td>0</td><td>35</td><td>0</td><td>12892</td><td>12857</td><td>12822</td><td>12787</td><td>12752</td><td>12717</td><td>12612</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 71)\n",
       "┌───────────┬──────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ product_i ┆ store_id ┆ log_units ┆ product_g ┆ … ┆ h4_ewm_3y ┆ h5_ewm_3y ┆ h6_ewm_3y ┆ h7_ewm_3y │\n",
       "│ d         ┆ ---      ┆ _sold     ┆ roup      ┆   ┆ _log_unit ┆ _log_unit ┆ _log_unit ┆ _log_unit │\n",
       "│ ---       ┆ u32      ┆ ---       ┆ ---       ┆   ┆ s_sold    ┆ s_sold    ┆ s_sold    ┆ s_sold    │\n",
       "│ u32       ┆          ┆ u32       ┆ u32       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---       │\n",
       "│           ┆          ┆           ┆           ┆   ┆ u32       ┆ u32       ┆ u32       ┆ u32       │\n",
       "╞═══════════╪══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 0         ┆ 0        ┆ 0         ┆ 0         ┆ … ┆ 12787     ┆ 12752     ┆ 12717     ┆ 12612     │\n",
       "└───────────┴──────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count nulls\n",
    "x.null_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b91d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_stage(\n",
    "    dataset: pl.LazyFrame,\n",
    "    interval: Tuple[date, date],\n",
    "    target_cols: List[str],\n",
    ") -> Tuple[pl.DataFrame, pl.DataFrame]:\n",
    "\n",
    "    x_df = dataset.filter(pl.col(\"date\").is_between(*interval)).collect()\n",
    "    date_df = pop_columns(x_df, [\"date\"])\n",
    "\n",
    "    return x_df, date_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8bcd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_interval = (date(2013, 1, 1), date(2016, 12, 31))\n",
    "save_split(dataset, train_interval, tag=\"train\", horizon=7)\n",
    "\n",
    "valid_interval = (date(2016, 12, 1), date(2017, 8, 15))\n",
    "save_split(dataset, valid_interval, tag=\"valid\", horizon=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c35e0ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (8, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>store_id</th><th>product_id</th><th>date</th><th>b</th><th>new</th></tr><tr><td>i64</td><td>i64</td><td>date</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>3</td><td>10</td><td>2020-01-01</td><td>1</td><td>null</td></tr><tr><td>3</td><td>10</td><td>2020-01-02</td><td>2</td><td>1</td></tr><tr><td>3</td><td>10</td><td>2020-01-03</td><td>null</td><td>null</td></tr><tr><td>3</td><td>10</td><td>2020-01-04</td><td>null</td><td>null</td></tr><tr><td>3</td><td>10</td><td>2020-01-06</td><td>5</td><td>null</td></tr><tr><td>3</td><td>10</td><td>2020-01-07</td><td>6</td><td>5</td></tr><tr><td>2</td><td>10</td><td>2020-01-01</td><td>1</td><td>null</td></tr><tr><td>2</td><td>10</td><td>2020-01-02</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (8, 5)\n",
       "┌──────────┬────────────┬────────────┬──────┬──────┐\n",
       "│ store_id ┆ product_id ┆ date       ┆ b    ┆ new  │\n",
       "│ ---      ┆ ---        ┆ ---        ┆ ---  ┆ ---  │\n",
       "│ i64      ┆ i64        ┆ date       ┆ i64  ┆ i64  │\n",
       "╞══════════╪════════════╪════════════╪══════╪══════╡\n",
       "│ 3        ┆ 10         ┆ 2020-01-01 ┆ 1    ┆ null │\n",
       "│ 3        ┆ 10         ┆ 2020-01-02 ┆ 2    ┆ 1    │\n",
       "│ 3        ┆ 10         ┆ 2020-01-03 ┆ null ┆ null │\n",
       "│ 3        ┆ 10         ┆ 2020-01-04 ┆ null ┆ null │\n",
       "│ 3        ┆ 10         ┆ 2020-01-06 ┆ 5    ┆ null │\n",
       "│ 3        ┆ 10         ┆ 2020-01-07 ┆ 6    ┆ 5    │\n",
       "│ 2        ┆ 10         ┆ 2020-01-01 ┆ 1    ┆ null │\n",
       "│ 2        ┆ 10         ┆ 2020-01-02 ┆ null ┆ null │\n",
       "└──────────┴────────────┴────────────┴──────┴──────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "alpha = 0.9\n",
    "window = 2\n",
    "\n",
    "dates = [\n",
    "    \"2020-01-01\",\n",
    "    \"2020-01-02\",\n",
    "    \"2020-01-03\",\n",
    "    \"2020-01-04\",\n",
    "    \"2020-01-06\",\n",
    "    \"2020-01-07\",\n",
    "    \"2020-01-01\",\n",
    "    \"2020-01-02\",\n",
    "]\n",
    "\n",
    "df = pl.DataFrame({\n",
    "    \"store_id\": [3,3,3,3,3,3,2,2],\n",
    "    \"product_id\": [10,10,10,10,10,10,10,10],\n",
    "    \"date\": dates,\n",
    "    \"b\": [1,2,None,None,5,6,1,None],\n",
    "}).with_columns(\n",
    "    pl.col(\"date\").str.strptime(pl.Date).set_sorted()\n",
    ")\n",
    "\n",
    "# [\n",
    "#     # pl.col(\"b\"). #.last()#.shift(-1).first().fill_null(0)\n",
    "# ]\n",
    "df.with_columns(\n",
    "    pl.col(\"b\")\n",
    "        # .rolling(\"date\", period=\"2d\", closed=\"left\", offset=\"0d\")\n",
    "        # .list.mean()\n",
    "        .rolling_min(2)\n",
    "        .over([\"product_id\", \"store_id\"], order_by=\"date\")\n",
    "        .alias(\"new\")\n",
    "        # .rolling(\"date\", period=\"2d\", closed=\"left\", offset=\"0d\")\n",
    "        # .mean()\n",
    "        #     pl.col(\"b\").shift(-1)  # Shift to get the next value in the group\n",
    "        # )\n",
    "        # .shift(-1)  # Shift to get the next value in the group\n",
    "        \n",
    "        # .over([\"product_id\", \"store_id\"])\n",
    "        # .ewm_mean(alpha=0.9, adjust=True)\n",
    "        # .alias(\"new\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0741bb52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (8, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>product_id</th><th>store_id</th><th>date</th><th>b</th><th>c</th><th>b_rolling_window</th></tr><tr><td>i64</td><td>i64</td><td>date</td><td>list[i64]</td><td>list[i64]</td><td>f64</td></tr></thead><tbody><tr><td>10</td><td>2</td><td>2020-01-01</td><td>[1]</td><td>[1]</td><td>1.0</td></tr><tr><td>10</td><td>2</td><td>2020-01-02</td><td>[1, 2]</td><td>[1, 1]</td><td>1.5</td></tr><tr><td>10</td><td>3</td><td>2020-01-01</td><td>[1]</td><td>[1]</td><td>1.0</td></tr><tr><td>10</td><td>3</td><td>2020-01-02</td><td>[1, 2]</td><td>[1, 1]</td><td>1.5</td></tr><tr><td>10</td><td>3</td><td>2020-01-03</td><td>[2, 3]</td><td>[1, 1]</td><td>2.5</td></tr><tr><td>10</td><td>3</td><td>2020-01-04</td><td>[3, 4]</td><td>[1, 1]</td><td>3.5</td></tr><tr><td>10</td><td>3</td><td>2020-01-06</td><td>[5]</td><td>[1]</td><td>5.0</td></tr><tr><td>10</td><td>3</td><td>2020-01-07</td><td>[5, 6]</td><td>[1, 1]</td><td>5.5</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (8, 6)\n",
       "┌────────────┬──────────┬────────────┬───────────┬───────────┬──────────────────┐\n",
       "│ product_id ┆ store_id ┆ date       ┆ b         ┆ c         ┆ b_rolling_window │\n",
       "│ ---        ┆ ---      ┆ ---        ┆ ---       ┆ ---       ┆ ---              │\n",
       "│ i64        ┆ i64      ┆ date       ┆ list[i64] ┆ list[i64] ┆ f64              │\n",
       "╞════════════╪══════════╪════════════╪═══════════╪═══════════╪══════════════════╡\n",
       "│ 10         ┆ 2        ┆ 2020-01-01 ┆ [1]       ┆ [1]       ┆ 1.0              │\n",
       "│ 10         ┆ 2        ┆ 2020-01-02 ┆ [1, 2]    ┆ [1, 1]    ┆ 1.5              │\n",
       "│ 10         ┆ 3        ┆ 2020-01-01 ┆ [1]       ┆ [1]       ┆ 1.0              │\n",
       "│ 10         ┆ 3        ┆ 2020-01-02 ┆ [1, 2]    ┆ [1, 1]    ┆ 1.5              │\n",
       "│ 10         ┆ 3        ┆ 2020-01-03 ┆ [2, 3]    ┆ [1, 1]    ┆ 2.5              │\n",
       "│ 10         ┆ 3        ┆ 2020-01-04 ┆ [3, 4]    ┆ [1, 1]    ┆ 3.5              │\n",
       "│ 10         ┆ 3        ┆ 2020-01-06 ┆ [5]       ┆ [1]       ┆ 5.0              │\n",
       "│ 10         ┆ 3        ┆ 2020-01-07 ┆ [5, 6]    ┆ [1, 1]    ┆ 5.5              │\n",
       "└────────────┴──────────┴────────────┴───────────┴───────────┴──────────────────┘"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rolling('date', period='2d', closed='right', group_by=[\"product_id\", \"store_id\"]).agg(\n",
    "    pl.exclude(\"date\"),\n",
    "    pl.col(\"b\").mean().alias(\"b_rolling_window\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec05c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (8, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>product_id</th><th>store_id</th><th>b</th></tr><tr><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>10</td><td>2</td><td>1</td></tr><tr><td>10</td><td>2</td><td>1</td></tr><tr><td>10</td><td>3</td><td>1</td></tr><tr><td>10</td><td>3</td><td>2</td></tr><tr><td>10</td><td>3</td><td>2</td></tr><tr><td>10</td><td>3</td><td>2</td></tr><tr><td>10</td><td>3</td><td>5</td></tr><tr><td>10</td><td>3</td><td>6</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (8, 3)\n",
       "┌────────────┬──────────┬─────┐\n",
       "│ product_id ┆ store_id ┆ b   │\n",
       "│ ---        ┆ ---      ┆ --- │\n",
       "│ i64        ┆ i64      ┆ i64 │\n",
       "╞════════════╪══════════╪═════╡\n",
       "│ 10         ┆ 2        ┆ 1   │\n",
       "│ 10         ┆ 2        ┆ 1   │\n",
       "│ 10         ┆ 3        ┆ 1   │\n",
       "│ 10         ┆ 3        ┆ 2   │\n",
       "│ 10         ┆ 3        ┆ 2   │\n",
       "│ 10         ┆ 3        ┆ 2   │\n",
       "│ 10         ┆ 3        ┆ 5   │\n",
       "│ 10         ┆ 3        ┆ 6   │\n",
       "└────────────┴──────────┴─────┘"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.group_by([\"product_id\", \"store_id\"]).agg(\n",
    "    pl.col(\"b\").fill_null(strategy=\"forward\")#.over(\"date\")\n",
    ").explode(\"b\")\n",
    "# df.fill_null(strategy=\"backward\").over([\"product_id\", \"store_id\"], order_by=\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa42f316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1.909090909090909, 2.8918918918918917]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([1, 2, 3])\n",
    "alpha = 0.9\n",
    "\n",
    "ewm = [\n",
    "    x[0],\n",
    "    (2*1 + 1*0.1) / (1 + 0.1),\n",
    "    (3*1 + 2*0.1 + 1*0.01) / (1 + 0.1 + 0.01)\n",
    "]\n",
    "print(ewm)\n",
    "# [1.0, 1.9090909090909092, 2.891891891891892]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "operation-research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
